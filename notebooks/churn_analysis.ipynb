{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc7101c",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction Analysis\n",
    "\n",
    "## üìä Business Problem\n",
    "This notebook analyzes customer churn for a telecommunications company. Our goal is to:\n",
    "- Understand patterns in customer behavior\n",
    "- Build predictive models to identify at-risk customers\n",
    "- Provide actionable insights for customer retention strategies\n",
    "\n",
    "## üìã Analysis Outline\n",
    "1. **Data Loading & Overview**\n",
    "2. **Exploratory Data Analysis (EDA)**\n",
    "3. **Data Preprocessing**\n",
    "4. **Model Training & Evaluation**\n",
    "5. **Feature Importance Analysis**\n",
    "6. **Business Insights & Recommendations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a81c4",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3cf973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import xgboost as xgb\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÅ Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3feb51",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Overview\n",
    "\n",
    "First, let's load the dataset and get a basic understanding of our data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acc6c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Note: Download the Telco Customer Churn dataset and place it in the data/ folder\n",
    "data_path = '../data/Telco-Customer-Churn.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"üìä Dataset shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Dataset not found!\")\n",
    "    print(\"Please download the Telco Customer Churn dataset from:\")\n",
    "    print(\"https://www.kaggle.com/datasets/blastchar/telco-customer-churn\")\n",
    "    print(\"And place it in the ../data/ folder as 'Telco-Customer-Churn.csv'\")\n",
    "    \n",
    "    # Create sample data for demonstration\n",
    "    print(\"\\nüìù Creating sample data for demonstration...\")\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'customerID': [f'ID_{i:04d}' for i in range(n_samples)],\n",
    "        'gender': np.random.choice(['Male', 'Female'], n_samples),\n",
    "        'SeniorCitizen': np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),\n",
    "        'Partner': np.random.choice(['Yes', 'No'], n_samples),\n",
    "        'Dependents': np.random.choice(['Yes', 'No'], n_samples),\n",
    "        'tenure': np.random.randint(1, 73, n_samples),\n",
    "        'PhoneService': np.random.choice(['Yes', 'No'], n_samples, p=[0.9, 0.1]),\n",
    "        'InternetService': np.random.choice(['DSL', 'Fiber optic', 'No'], n_samples),\n",
    "        'Contract': np.random.choice(['Month-to-month', 'One year', 'Two year'], n_samples),\n",
    "        'PaymentMethod': np.random.choice(['Electronic check', 'Mailed check', 'Bank transfer (automatic)', 'Credit card (automatic)'], n_samples),\n",
    "        'MonthlyCharges': np.random.uniform(18, 120, n_samples).round(2),\n",
    "        'TotalCharges': np.random.uniform(18, 8500, n_samples).round(2),\n",
    "        'Churn': np.random.choice(['Yes', 'No'], n_samples, p=[0.27, 0.73])\n",
    "    })\n",
    "    print(f\"üìä Sample dataset created with shape: {df.shape}\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"\\nüìã Dataset Info:\")\n",
    "print(f\"Rows: {df.shape[0]:,}\")\n",
    "print(f\"Columns: {df.shape[1]:,}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ef5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"üîç First 5 rows of the dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nüìä Dataset columns and data types:\")\n",
    "display(df.dtypes.to_frame(name='Data Type'))\n",
    "\n",
    "print(\"\\nüìà Basic statistics:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1824cd75",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let's explore the data to understand patterns, distributions, and relationships between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedde8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"üîç Missing Values Analysis:\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Percentage': missing_percentage\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "if missing_df['Missing Count'].sum() == 0:\n",
    "    print(\"‚úÖ No missing values found!\")\n",
    "\n",
    "# Churn distribution analysis\n",
    "print(f\"\\nüìä Churn Distribution:\")\n",
    "churn_counts = df['Churn'].value_counts()\n",
    "churn_percentage = df['Churn'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"Total customers: {len(df):,}\")\n",
    "for category, count in churn_counts.items():\n",
    "    pct = churn_percentage[category]\n",
    "    print(f\"{category}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Visualize churn distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Count plot\n",
    "sns.countplot(data=df, x='Churn', ax=ax1)\n",
    "ax1.set_title('Customer Churn Distribution', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Churn Status')\n",
    "ax1.set_ylabel('Number of Customers')\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, v in enumerate(churn_counts.values):\n",
    "    ax1.text(i, v + 50, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(churn_counts.values, labels=churn_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Churn Rate Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/plots/churn_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50ddbe5",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "Prepare the data for machine learning by encoding categorical variables and scaling numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb84f297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Remove customer ID as it's not useful for prediction\n",
    "if 'customerID' in df_processed.columns:\n",
    "    df_processed = df_processed.drop('customerID', axis=1)\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_columns = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_columns = df_processed.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Remove target variable from categorical columns\n",
    "if 'Churn' in categorical_columns:\n",
    "    categorical_columns.remove('Churn')\n",
    "\n",
    "print(f\"üìä Feature Analysis:\")\n",
    "print(f\"Categorical features ({len(categorical_columns)}): {categorical_columns}\")\n",
    "print(f\"Numerical features ({len(numerical_columns)}): {numerical_columns}\")\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for column in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df_processed[column] = le.fit_transform(df_processed[column])\n",
    "    label_encoders[column] = le\n",
    "    print(f\"‚úÖ Encoded {column}: {list(le.classes_)}\")\n",
    "\n",
    "# Encode target variable\n",
    "target_encoder = LabelEncoder()\n",
    "df_processed['Churn'] = target_encoder.fit_transform(df_processed['Churn'])\n",
    "print(f\"‚úÖ Target encoded: {list(target_encoder.classes_)} -> {list(target_encoder.transform(target_encoder.classes_))}\")\n",
    "\n",
    "# Prepare features and target\n",
    "X = df_processed.drop('Churn', axis=1)\n",
    "y = df_processed['Churn']\n",
    "\n",
    "print(f\"\\nüìà Final dataset shape:\")\n",
    "print(f\"Features (X): {X.shape}\")\n",
    "print(f\"Target (y): {y.shape}\")\n",
    "print(f\"Feature names: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9725f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"üìä Data Split:\")\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"Training churn rate: {y_train.mean():.3f}\")\n",
    "print(f\"Test churn rate: {y_test.mean():.3f}\")\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Scale only numerical columns\n",
    "numerical_indices = [X.columns.get_loc(col) for col in numerical_columns if col in X.columns]\n",
    "if numerical_indices:\n",
    "    X_train_scaled.iloc[:, numerical_indices] = scaler.fit_transform(X_train.iloc[:, numerical_indices])\n",
    "    X_test_scaled.iloc[:, numerical_indices] = scaler.transform(X_test.iloc[:, numerical_indices])\n",
    "    print(f\"‚úÖ Scaled {len(numerical_indices)} numerical features\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No numerical features to scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9388442d",
   "metadata": {},
   "source": [
    "## 5. Model Training & Evaluation\n",
    "\n",
    "Train multiple models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa2462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(\"ü§ñ Training Models...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîÑ Training {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'roc_auc': roc_auc,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    trained_models[name] = model\n",
    "    \n",
    "    print(f\"‚úÖ {name} - Accuracy: {accuracy:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Create results comparison\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': results.keys(),\n",
    "    'Accuracy': [results[model]['accuracy'] for model in results.keys()],\n",
    "    'ROC-AUC': [results[model]['roc_auc'] for model in results.keys()]\n",
    "})\n",
    "\n",
    "print(f\"\\nüìä Model Performance Summary:\")\n",
    "display(results_df.sort_values('ROC-AUC', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e5d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation for the best model\n",
    "best_model_name = results_df.sort_values('ROC-AUC', ascending=False).iloc[0]['Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "print(f\"üèÜ Best Model: {best_model_name}\")\n",
    "print(f\"üìä Detailed Classification Report:\")\n",
    "print(classification_report(y_test, best_predictions))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Churn', 'Churn'], \n",
    "            yticklabels=['No Churn', 'Churn'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.savefig('../outputs/plots/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance Analysis\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Tree-based models\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # Linear models\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': abs(best_model.coef_[0])\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(10)\n",
    "sns.barplot(data=top_features, y='feature', x='importance', palette='viridis')\n",
    "plt.title(f'Top 10 Feature Importance - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/plots/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ Top 5 Most Important Features:\")\n",
    "for i, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"{row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "model_path = f'../outputs/models/best_churn_model_{best_model_name.lower().replace(\" \", \"_\")}.joblib'\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"\\nüíæ Best model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd79794",
   "metadata": {},
   "source": [
    "## 6. Business Insights & Recommendations\n",
    "\n",
    "Based on our analysis, here are the key insights and actionable recommendations for reducing customer churn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Insights Summary\n",
    "insights = {\n",
    "    'Model Performance': f'{best_model_name} achieved {results[best_model_name][\"roc_auc\"]:.1%} ROC-AUC score',\n",
    "    'Churn Rate': f'{(y.mean())*100:.1f}% of customers churned',\n",
    "    'Model Accuracy': f'{results[best_model_name][\"accuracy\"]:.1%} prediction accuracy'\n",
    "}\n",
    "\n",
    "print(\"üéØ KEY BUSINESS INSIGHTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for key, value in insights.items():\n",
    "    print(f\"‚Ä¢ {key}: {value}\")\n",
    "\n",
    "print(f\"\\nüìà TOP CHURN DRIVERS:\")\n",
    "for i, row in feature_importance.head(3).iterrows():\n",
    "    feature_name = row['feature']\n",
    "    importance = row['importance']\n",
    "    print(f\"‚Ä¢ {feature_name}: {importance:.3f} importance score\")\n",
    "\n",
    "print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "recommendations = [\n",
    "    \"üéØ Target high-risk customers identified by the model for retention campaigns\",\n",
    "    \"üìû Implement proactive customer service for customers with top risk factors\",\n",
    "    \"üí∞ Consider pricing strategies for customers with high monthly charges\",\n",
    "    \"üìã Review contract terms to encourage longer-term commitments\",\n",
    "    \"üîß Improve service quality in areas identified as churn drivers\",\n",
    "    \"üìä Monitor model performance monthly and retrain with new data\"\n",
    "]\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(rec)\n",
    "\n",
    "print(f\"\\nüìä BUSINESS IMPACT ESTIMATION:\")\n",
    "total_customers = len(df)\n",
    "current_churn_rate = y.mean()\n",
    "potential_savings = f\"Reducing churn by 10% could save ~{int(total_customers * current_churn_rate * 0.1)} customers\"\n",
    "print(f\"‚Ä¢ {potential_savings}\")\n",
    "print(f\"‚Ä¢ Model can help prioritize retention efforts for maximum ROI\")\n",
    "\n",
    "print(f\"\\nüéâ PROJECT SUMMARY:\")\n",
    "print(f\"‚úÖ Built and validated {len(models)} machine learning models\")\n",
    "print(f\"‚úÖ Achieved {results[best_model_name]['accuracy']:.1%} accuracy with {best_model_name}\")\n",
    "print(f\"‚úÖ Identified key churn drivers for business action\")\n",
    "print(f\"‚úÖ Created interpretable model for stakeholder understanding\")\n",
    "print(f\"‚úÖ Saved best model for future predictions\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
